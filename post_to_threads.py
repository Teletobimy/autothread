"""
âœ¨ í”¼ë”ë¦° (PYDERIN): í”¼ë¶€ ê³¼í•™ ê¸°ë°˜ PDRN ì „ë¬¸ ë”ë§ˆ ì†”ë£¨ì…˜

1. ë¸Œëœë“œ ê°œìš” ë° ë¯¸ì…˜

í•­ëª©ë‚´ìš©ë¸Œëœë“œëª…PYDERIN (í”¼ë”ë¦°)ë¸Œëœë“œ ì •ì²´ì„±í”¼ë¶€ ë³¸ì—°ì˜ ê±´ê°•í•œ íšŒë³µì— ì§‘ì¤‘í•˜ëŠ” PDRN ì „ë¬¸ ë”ë§ˆ ì½”ìŠ¤ë©”í‹± ë¸Œëœë“œ (Neo-Regeneration)í•µì‹¬ ë¯¸ì…˜ì˜ë£Œ ì „ë¬¸ì„±ê³¼ ì„±ë¶„ íˆ¬ëª…ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ, ì¶•ì ëœ í”¼ë¶€ ê³¼í•™ ê¸°ìˆ ë ¥ì„ ì¼ìƒ ì† í”¼ë¶€ ì¼€ì–´ë¡œ ì—°ê²°í•˜ì—¬ ê±´ê°•í•œ ì•„ë¦„ë‹¤ì›€ì„ ì™„ì„±í•©ë‹ˆë‹¤.2. í”¼ë”ë¦°ì˜ ì°¨ë³„í™”ëœ í•µì‹¬ ê°€ì¹˜ (The Core Value)

í”¼ë”ë¦°ì€ ë‹¨ìˆœí•œ í™”ì¥í’ˆì„ ë„˜ì–´, **'ê³¼í•™ì ìœ¼ë¡œ ê²€ì¦ëœ í”¼ë¶€ íšŒë³µ ì†”ë£¨ì…˜'**ì„ ì œê³µí•©ë‹ˆë‹¤.

ğŸ¥‡ ê³ ìˆœë„ PDRN (í•µì‹¬ ì„±ë¶„)

ì„±ë¶„ íˆ¬ëª…ì„±: ëŒ€í•œë¯¼êµ­ ì²œì—°ì–´ì—ì„œ ì—„ì„ í•˜ì—¬ ì¶”ì¶œí•œ 99% ê³ ìˆœë„ PDRNë§Œì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

PDRNì˜ ì—­í• : í”¼ë¶€ì— ìƒˆë¡œìš´ ìƒëª… ì—ë„ˆì§€ë¥¼ ë¶ˆì–´ë„£ì–´ í”¼ë¶€ ì¬ìƒì˜ ë³¸ì§ˆì— ì§‘ì¤‘í•˜ê³ , ì†ìƒëœ í”¼ë¶€ì˜ ê±´ê°•í•œ íšŒë³µì„ ë•ìŠµë‹ˆë‹¤.

ğŸ”¬ ì´ˆì €ë¶„ì ê³¼í•™ ê¸°ìˆ  (ì „ë‹¬ë ¥)

97kDa ì´ˆì €ë¶„ì ê¸°ìˆ : ìœ íš¨ ì„±ë¶„ì¸ PDRNì„ í”¼ë¶€ ê¹Šì€ ê³³ê¹Œì§€ íš¨ìœ¨ì ìœ¼ë¡œ ì „ë‹¬í•˜ê¸° ìœ„í•´ ì´ˆì €ë¶„ìí™” ê¸°ìˆ ì„ ì ìš©í–ˆìŠµë‹ˆë‹¤.

ìµœì ì˜ í¡ìˆ˜: í”¼ë¶€ ì»¨ë””ì…˜ì„ ê· í˜• ìˆê²Œ ê´€ë¦¬í•˜ê³ , ì„±ë¶„ì˜ íš¨ëŠ¥ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.

ğŸ§‘â€âš•ï¸ ì˜ë£Œ ê¸°ë°˜ ì†”ë£¨ì…˜ (ì‹ ë¢°ì„±)

ê²€ì¦ëœ ë°ì´í„°: êµ­ë‚´ì™¸ ìš°ìˆ˜ ì—°êµ¬ê¸°ê´€ê³¼ í˜‘ë ¥í•˜ì—¬ ê²€ì¦ëœ ì„ìƒ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì œí’ˆì„ ê°œë°œí•©ë‹ˆë‹¤.

ì „ë¬¸ì„±: ì˜ë£Œ í˜„ì¥ì—ì„œ ì•ˆì •ì„±ê³¼ íšŒë³µë ¥ì´ ê²€ì¦ëœ ê¸°ìˆ ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ, ì¼ìƒì—ì„œë„ ì „ë¬¸ì ì¸ ë”ë§ˆ ì†”ë£¨ì…˜ì„ ê²½í—˜í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.

3. ì£¼ìš” ì œí’ˆ ë¼ì¸ì—… (ì˜ˆì‹œ)

í”¼ë”ë¦°ì€ ì „ë¬¸ì ì¸ ê¸°ìˆ ë ¥ì„ ë‹´ì•„ í”¼ë¶€ ê³ ë¯¼ë³„ ìµœì í™”ëœ í† íƒˆ ìŠ¤í‚¨ì¼€ì–´ ë¼ì¸ì„ ì œê³µí•©ë‹ˆë‹¤.

Hospital Line: ì˜ë£Œ í˜„ì¥ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì „ë¬¸ ì œí’ˆêµ° (ë³‘ì› ì „ìš©)

Daily Care Line: ì¼ìƒ ì† í”¼ë¶€ ì»¨ë””ì…˜ì„ ê´€ë¦¬í•˜ëŠ” ë°ì¼ë¦¬ ì†”ë£¨ì…˜

PDRN íƒ„ë ¥ í• ì•°í”Œ: ê³ ìˆœë„ PDRNì„ ë‹´ì•„ ì§‘ì¤‘ì ì¸ íƒ„ë ¥ ë° ì¬ìƒ ê´€ë¦¬ë¥¼ ë•ëŠ” í•µì‹¬ ì œí’ˆ.

PDRN íƒ„ë ¥ í¬ë¦¼: í”¼ë¶€ ì¥ë²½ ê°•í™” ë° íƒ„ë ¥ ìœ ì§€ì— ë„ì›€ì„ ì£¼ëŠ” ê³ ë³´ìŠµ í¬ë¦¼.

PDRN ë§ˆìŠ¤í¬íŒ©: ì§‘ì¤‘ì ì¸ ì˜ì–‘ ê³µê¸‰ ë° í”¼ë¶€ íšŒë³µì„ ìœ„í•œ ë§ˆìŠ¤í¬íŒ© (ì†/ë°œ ë§ˆìŠ¤í¬íŒ© í¬í•¨).

PDRN í´ë Œì§•: ìˆœí•˜ë©´ì„œë„ íš¨ê³¼ì ì¸ ì„¸ì •ì„ ë•ëŠ” ê±°í’ˆ í´ë Œì €.



ì´ ë¶€ë¶„ì„ AI AX ì „ë¬¸ê°€ë¡œ ë°”ê¿€ê±´ë° ë‹¤ì‹œ ì˜ í”„ë¡¬í”„íŠ¸ë¡œ ì“¸ ìˆ˜ ìˆê²Œ ì •ë¦¬í•´ì„œ ë‚˜í•œí…Œ ì•Œë ¤ì¤˜



ì£¼ì œ => AI AX ì „ë¬¸ê°€ ê¸°ì¤€ì—ì„œì˜ ê´€ì :

ë°°ê²½ : K-ë·°í‹° ë¸Œëœë“œì—ì„œ íšŒì‚¬ ë‚´ë¶€ ì—…ë¬´ë¥¼ ê°•ë ¥í•˜ê²Œ AX ì¶”ì§„ ì¤‘ì¸ ìƒí™©

1) ì§€ê¸ˆê¹Œì§€ ì§„í–‰ ì™„ë£Œ
 - 


"""


import json
import os
import time
import sys
from typing import Callable, List, Optional

import requests
from dotenv import load_dotenv
from openai import OpenAI

try:
    from google import genai as google_genai
    GOOGLE_GENAI_AVAILABLE = True
except ImportError:
    GOOGLE_GENAI_AVAILABLE = False
    google_genai = None

# Windows ì½˜ì†” UTF-8 ì¸ì½”ë”© ì„¤ì •
if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except AttributeError:
        # Python 3.6 ì´í•˜ ë²„ì „ì„ ìœ„í•œ ëŒ€ì²´ ë°©ë²•
        import codecs
        sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
        sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

BASE = "https://graph.threads.net/v1.0"

Logger = Optional[Callable[[str], None]]


def _emit(message: str, logger: Logger = None) -> None:
    """Helper to send messages to either stdout or a provided logger."""
    if logger:
        logger(message)
    else:
        print(message)

def get_token():
    """í™˜ê²½ ë³€ìˆ˜ì—ì„œ í† í°ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    token = os.getenv('LONG_LIVED_ACCESS_TOKEN')
    if not token:
        raise ValueError("LONG_LIVED_ACCESS_TOKENì´ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    return token.strip().strip('"').strip("'")  # ë”°ì˜´í‘œ ì œê±°

def get_google_api_key():
    """í™˜ê²½ ë³€ìˆ˜ì—ì„œ Google API í‚¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    api_key = os.getenv('GOOGLE_API_KEY')
    if not api_key:
        raise ValueError("GOOGLE_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    return api_key.strip().strip('"').strip("'")

def _build_prompt(topic=None, style="engaging", max_length=500):
    """í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."""
    if topic:
        prompt = f"""Threadsì— ê²Œì‹œí•  {style}í•œ í…ìŠ¤íŠ¸ ì½˜í…ì¸ ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì£¼ì œ: {topic}

ìš”êµ¬ì‚¬í•­:
- Threads í”Œë«í¼ì— ì í•©í•œ ê°„ê²°í•˜ê³  ë§¤ë ¥ì ì¸ ì½˜í…ì¸ 
- ìµœëŒ€ {max_length}ì ì´ë‚´
- ìì—°ìŠ¤ëŸ½ê³  ì½ê¸° ì‰¬ìš´ ë¬¸ì²´
- í•µì‹¬ ë©”ì‹œì§€ê°€ ëª…í™•í•˜ê²Œ ì „ë‹¬ë˜ë„ë¡ ì‘ì„±

í…ìŠ¤íŠ¸ë§Œ ì‘ì„±í•˜ê³ , ë”°ì˜´í‘œë‚˜ ì„¤ëª… ì—†ì´ ì½˜í…ì¸ ë§Œ ë°˜í™˜í•´ì£¼ì„¸ìš”."""
    else:
        prompt = f"""Threadsì— ê²Œì‹œí•  {style}í•œ í…ìŠ¤íŠ¸ ì½˜í…ì¸ ë¥¼ ììœ ë¡­ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
- Threads í”Œë«í¼ì— ì í•©í•œ ê°„ê²°í•˜ê³  ë§¤ë ¥ì ì¸ ì½˜í…ì¸ 
- ìµœëŒ€ {max_length}ì ì´ë‚´
- ìì—°ìŠ¤ëŸ½ê³  ì½ê¸° ì‰¬ìš´ ë¬¸ì²´
- í¥ë¯¸ë¡­ê³  ì°¸ì—¬ë¥¼ ìœ ë„í•˜ëŠ” ë‚´ìš©

í…ìŠ¤íŠ¸ë§Œ ì‘ì„±í•˜ê³ , ë”°ì˜´í‘œë‚˜ ì„¤ëª… ì—†ì´ ì½˜í…ì¸ ë§Œ ë°˜í™˜í•´ì£¼ì„¸ìš”."""
    return prompt

def generate_text_with_gemini(topic=None, style="engaging", max_length=500, logger: Logger = None):
    """
    Google Geminië¥¼ ì‚¬ìš©í•˜ì—¬ Threadsìš© í…ìŠ¤íŠ¸ ì½˜í…ì¸ ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        topic (str, optional): ìƒì„±í•  ì½˜í…ì¸ ì˜ ì£¼ì œ. Noneì´ë©´ ìë™ ìƒì„±
        style (str): ì½˜í…ì¸  ìŠ¤íƒ€ì¼ (ê¸°ë³¸ê°’: "engaging")
        max_length (int): ìµœëŒ€ ë¬¸ì ê¸¸ì´ (ê¸°ë³¸ê°’: 500)
        logger (Logger, optional): ë¡œê·¸ í•¨ìˆ˜
    
    Returns:
        str: ìƒì„±ëœ Threads í…ìŠ¤íŠ¸ ì½˜í…ì¸ 
    """
    if not GOOGLE_GENAI_AVAILABLE:
        raise ImportError("google-genai ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install google-genaië¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    
    # í™˜ê²½ ë³€ìˆ˜ì—ì„œ Google API í‚¤ ë¡œë“œ
    api_key = get_google_api_key()
    
    # Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    client = google_genai.Client(api_key=api_key)
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ê²°í•©
    system_prompt = "You are a creative content writer specializing in social media posts for Threads platform."
    user_prompt = _build_prompt(topic=topic, style=style, max_length=max_length)
    full_prompt = f"{system_prompt}\n\n{user_prompt}"
    
    try:
        # Gemini API í˜¸ì¶œ
        response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=full_prompt
        )
        
        # ìƒì„±ëœ ì½˜í…ì¸  ì¶”ì¶œ
        content = response.text.strip()
        
        # ë”°ì˜´í‘œ ì œê±° (ìˆëŠ” ê²½ìš°)
        if content.startswith('"') and content.endswith('"'):
            content = content[1:-1]
        elif content.startswith("'") and content.endswith("'"):
            content = content[1:-1]
        
        _emit(f"âœ… Gemini ì½˜í…ì¸  ìƒì„± ì™„ë£Œ ({len(content)}ì)", logger)
        return content
        
    except Exception as e:
        _emit(f"âŒ Gemini ì½˜í…ì¸  ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", logger)
        raise

def generate_text_with_gpt(topic=None, style="engaging", max_length=500, logger: Logger = None):
    """
    GPTë¥¼ ì‚¬ìš©í•˜ì—¬ Threadsìš© í…ìŠ¤íŠ¸ ì½˜í…ì¸ ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        topic (str, optional): ìƒì„±í•  ì½˜í…ì¸ ì˜ ì£¼ì œ. Noneì´ë©´ ìë™ ìƒì„±
        style (str): ì½˜í…ì¸  ìŠ¤íƒ€ì¼ (ê¸°ë³¸ê°’: "engaging")
        max_length (int): ìµœëŒ€ ë¬¸ì ê¸¸ì´ (ê¸°ë³¸ê°’: 500)
        logger (Logger, optional): ë¡œê·¸ í•¨ìˆ˜
    
    Returns:
        str: ìƒì„±ëœ Threads í…ìŠ¤íŠ¸ ì½˜í…ì¸ 
    """
    # í™˜ê²½ ë³€ìˆ˜ì—ì„œ OpenAI API í‚¤ ë¡œë“œ
    api_key = os.getenv('OPENAI_API_KEY')
    model = 'gpt-4o'  # gpt-5ëŠ” ì•„ì§ ì¶œì‹œë˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ gpt-4o ì‚¬ìš©
    
    if not api_key:
        raise ValueError("OPENAI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    client = OpenAI(api_key=api_key.strip().strip('"').strip("'"))
    
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = _build_prompt(topic=topic, style=style, max_length=max_length)
    
    try:
        # GPT API í˜¸ì¶œ
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are a creative content writer specializing in social media posts for Threads platform."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=200
        )
        
        # ìƒì„±ëœ ì½˜í…ì¸  ì¶”ì¶œ
        content = response.choices[0].message.content.strip()
        
        # ë”°ì˜´í‘œ ì œê±° (ìˆëŠ” ê²½ìš°)
        if content.startswith('"') and content.endswith('"'):
            content = content[1:-1]
        elif content.startswith("'") and content.endswith("'"):
            content = content[1:-1]
        
        _emit(f"âœ… GPT ì½˜í…ì¸  ìƒì„± ì™„ë£Œ ({len(content)}ì)", logger)
        return content
        
    except Exception as e:
        _emit(f"âŒ GPT ì½˜í…ì¸  ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", logger)
        raise

def generate_text_with_ai(
    model="gpt-4o",
    topic=None,
    style="engaging",
    max_length=500,
    logger: Logger = None
):
    """
    AI ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ Threadsìš© í…ìŠ¤íŠ¸ ì½˜í…ì¸ ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        model (str): ì‚¬ìš©í•  AI ëª¨ë¸ ("gpt-4o" ë˜ëŠ” "gemini-2.5-flash")
        topic (str, optional): ìƒì„±í•  ì½˜í…ì¸ ì˜ ì£¼ì œ. Noneì´ë©´ ìë™ ìƒì„±
        style (str): ì½˜í…ì¸  ìŠ¤íƒ€ì¼ (ê¸°ë³¸ê°’: "engaging")
        max_length (int): ìµœëŒ€ ë¬¸ì ê¸¸ì´ (ê¸°ë³¸ê°’: 500)
        logger (Logger, optional): ë¡œê·¸ í•¨ìˆ˜
    
    Returns:
        str: ìƒì„±ëœ Threads í…ìŠ¤íŠ¸ ì½˜í…ì¸ 
    """
    if model.startswith("gpt") or model == "gpt-4o":
        return generate_text_with_gpt(topic=topic, style=style, max_length=max_length, logger=logger)
    elif model.startswith("gemini") or model == "gemini-2.5-flash":
        return generate_text_with_gemini(topic=topic, style=style, max_length=max_length, logger=logger)
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤: {model}. 'gpt-4o' ë˜ëŠ” 'gemini-2.5-flash'ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")

def me(token=None):
    if token is None:
        token = get_token()
    r = requests.get(f"{BASE}/me", params={"fields":"id,username","access_token":token}, timeout=20)
    r.raise_for_status()
    return r.json()

def create_text_container(threads_user_id, text, token=None):
    if token is None:
        token = get_token()
    payload = {"media_type":"TEXT", "text": text, "access_token": token}
    r = requests.post(f"{BASE}/{threads_user_id}/threads",
                      headers={"Content-Type":"application/json"},
                      data=json.dumps(payload), timeout=30)
    r.raise_for_status()
    return r.json()["id"]  # creation_id

def publish_container(threads_user_id, creation_id, token=None):
    if token is None:
        token = get_token()
    r = requests.post(f"{BASE}/{threads_user_id}/threads_publish",
                      data={"creation_id": creation_id, "access_token": token}, timeout=20)
    r.raise_for_status()
    return r.json()["id"]  # ìµœì¢… media id

def get_permalink(media_id, token=None):
    if token is None:
        token = get_token()
    r = requests.get(f"{BASE}/{media_id}", params={"fields":"permalink","access_token":token}, timeout=20)
    r.raise_for_status()
    return r.json()["permalink"]

def _post_text_to_threads(threads_user_id: str, text: str, token: str, logger: Logger = None):
    """Create, publish, and return metadata for a single Threads post."""
    _emit("ğŸ“¦ ì»¨í…Œì´ë„ˆ ìƒì„± ì¤‘...", logger)
    creation_id = create_text_container(threads_user_id, text, token=token)

    _emit("ğŸš€ Threadsì— ê²Œì‹œ ì¤‘...", logger)
    media_id = publish_container(threads_user_id, creation_id, token=token)

    _emit("ğŸ”— Permalink ê°€ì ¸ì˜¤ëŠ” ì¤‘...", logger)
    permalink = get_permalink(media_id, token=token)

    return {
        "media_id": media_id,
        "creation_id": creation_id,
        "permalink": permalink,
        "text": text,
        "user_id": threads_user_id,
    }


def post_gpt_generated_text(
    topic=None,
    style="engaging",
    max_length=500,
    model="gpt-4o",
    token=None,
    logger: Logger = None
):
    """
    AI ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê³  Threadsì— ê²Œì‹œí•˜ëŠ” ì „ì²´ í”Œë¡œìš°ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    
    Args:
        topic (str, optional): AIê°€ ìƒì„±í•  ì½˜í…ì¸ ì˜ ì£¼ì œ
        style (str): ì½˜í…ì¸  ìŠ¤íƒ€ì¼
        max_length (int): ìµœëŒ€ ë¬¸ì ê¸¸ì´
        model (str): ì‚¬ìš©í•  AI ëª¨ë¸ ("gpt-4o" ë˜ëŠ” "gemini-2.5-flash")
        token (str, optional): Threads ì•¡ì„¸ìŠ¤ í† í° (Noneì´ë©´ .envì—ì„œ ì½ìŒ)
        logger (Logger, optional): ë¡œê·¸ í•¨ìˆ˜
    
    Returns:
        dict: ê²Œì‹œ ê²°ê³¼ (media_id, permalink ë“± í¬í•¨)
    """
    # 1ë‹¨ê³„: AIë¡œ í…ìŠ¤íŠ¸ ìƒì„±
    model_name = "GPT" if model.startswith("gpt") else "Gemini"
    _emit(f"ğŸ¤– {model_name}ë¡œ í…ìŠ¤íŠ¸ ìƒì„± ì¤‘...", logger)
    text = generate_text_with_ai(model=model, topic=topic, style=style, max_length=max_length, logger=logger)
    _emit(f"ìƒì„±ëœ í…ìŠ¤íŠ¸: {text[:100]}...", logger)

    # 2ë‹¨ê³„: Threads ì‚¬ìš©ì ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    if token is None:
        token = get_token()
    _emit("ğŸ“‹ Threads ì‚¬ìš©ì ì •ë³´ í™•ì¸ ì¤‘...", logger)
    user_info = me(token=token)
    threads_user_id = user_info["id"]
    username = user_info.get('username', 'N/A')
    _emit(f"ì‚¬ìš©ì ID: {threads_user_id} (@{username})", logger)

    # 3~5ë‹¨ê³„: ê²Œì‹œ ë° ë§í¬ ë°˜í™˜
    result = _post_text_to_threads(threads_user_id, text, token, logger=logger)

    _emit("\nâœ… ê²Œì‹œ ì™„ë£Œ!", logger)
    _emit(f"ğŸ“ Media ID: {result['media_id']}", logger)
    _emit(f"ğŸ”— Permalink: {result['permalink']}", logger)

    return result


def post_multiple_gpt_texts(
    topic=None,
    style="engaging",
    max_length=500,
    count=5,
    interval_seconds=60,
    model="gpt-4o",
    token=None,
    logger: Logger = None,
) -> List[dict]:
    """
    ì§€ì •ëœ íšŸìˆ˜ë§Œí¼ Threadsì— AI ìƒì„± ê²Œì‹œë¬¼ì„ ìˆœì°¨ì ìœ¼ë¡œ ì—…ë¡œë“œí•©ë‹ˆë‹¤.

    Args:
        topic (str, optional): ê° ê²Œì‹œë¬¼ì— ëŒ€í•œ í”„ë¡¬í”„íŠ¸ ì£¼ì œ.
        style (str): ì½˜í…ì¸  ìŠ¤íƒ€ì¼.
        max_length (int): ê²Œì‹œë¬¼ ìµœëŒ€ ê¸¸ì´.
        count (int): ê²Œì‹œí•  ê²Œì‹œë¬¼ ìˆ˜.
        interval_seconds (int): ê²Œì‹œ ì‚¬ì´ ì§€ì—°(ì´ˆ).
        model (str): ì‚¬ìš©í•  AI ëª¨ë¸ ("gpt-4o" ë˜ëŠ” "gemini-2.5-flash").
        token (str, optional): Threads ì•¡ì„¸ìŠ¤ í† í°.
        logger (callable, optional): ë¡œê·¸ ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•  ì½œë°±.

    Returns:
        List[dict]: ê° ê²Œì‹œë¬¼ì˜ ê²°ê³¼ ì •ë³´ ëª©ë¡.
    """
    if count < 1:
        raise ValueError("countëŠ” 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")

    if interval_seconds < 0:
        raise ValueError("interval_secondsëŠ” 0 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")

    if token is None:
        token = get_token()

    _emit("ğŸ“‹ Threads ì‚¬ìš©ì ì •ë³´ í™•ì¸ ì¤‘...", logger)
    user_info = me(token=token)
    threads_user_id = user_info["id"]
    username = user_info.get('username', 'N/A')
    _emit(f"ì‚¬ìš©ì ID: {threads_user_id} (@{username})", logger)

    model_name = "GPT" if model.startswith("gpt") else "Gemini"
    results = []

    for idx in range(count):
        _emit(f"\n===== ê²Œì‹œ {idx + 1}/{count} ì‹œì‘ =====", logger)
        _emit(f"ğŸ¤– {model_name}ë¡œ í…ìŠ¤íŠ¸ ìƒì„± ì¤‘...", logger)
        text = generate_text_with_ai(model=model, topic=topic, style=style, max_length=max_length, logger=logger)
        _emit(f"ìƒì„±ëœ í…ìŠ¤íŠ¸: {text[:100]}...", logger)

        result = _post_text_to_threads(threads_user_id, text, token, logger=logger)
        result["sequence"] = idx + 1
        result["username"] = username
        results.append(result)

        _emit(f"âœ… ê²Œì‹œ {idx + 1}/{count} ì™„ë£Œ! Permalink: {result['permalink']}", logger)

        if idx < count - 1 and interval_seconds > 0:
            _emit(f"â³ ë‹¤ìŒ ê²Œì‹œê¹Œì§€ {interval_seconds}ì´ˆ ëŒ€ê¸°í•©ë‹ˆë‹¤...", logger)
            time.sleep(interval_seconds)

    _emit("\nğŸ‰ ëª¨ë“  ê²Œì‹œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!", logger)
    return results

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Threadsì— AI ìƒì„± ì½˜í…ì¸ ë¥¼ ìë™ ê²Œì‹œí•©ë‹ˆë‹¤.")
    parser.add_argument("topic", nargs="?", help="AIê°€ ìƒì„±í•  ì½˜í…ì¸  ì£¼ì œ (ë¯¸ì…ë ¥ ì‹œ ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì‹¤í–‰)")
    parser.add_argument("--style", default="engaging", help="ì½˜í…ì¸  ìŠ¤íƒ€ì¼ (ê¸°ë³¸ê°’: engaging)")
    parser.add_argument("--max-length", type=int, default=500, help="ì½˜í…ì¸  ìµœëŒ€ ê¸€ì ìˆ˜ (ê¸°ë³¸ê°’: 500)")
    parser.add_argument("--count", type=int, default=5, help="ê²Œì‹œí•  ê²Œì‹œë¬¼ ìˆ˜ (ê¸°ë³¸ê°’: 5)")
    parser.add_argument("--interval", dest="interval_seconds", type=int, default=60, help="ê²Œì‹œ ê°„ê²©(ì´ˆ) (ê¸°ë³¸ê°’: 60)")
    parser.add_argument("--model", default="gpt-4o", choices=["gpt-4o", "gemini-2.5-flash"], help="ì‚¬ìš©í•  AI ëª¨ë¸ (ê¸°ë³¸ê°’: gpt-4o)")
    args = parser.parse_args()

    if args.topic:
        print(f"ğŸ¯ ì£¼ì œ: {args.topic}")
        print(f"ğŸ¤– ëª¨ë¸: {args.model}")
        print("=" * 60)
        post_multiple_gpt_texts(
            topic=args.topic,
            style=args.style,
            max_length=args.max_length,
            count=args.count,
            interval_seconds=args.interval_seconds,
            model=args.model,
        )
    else:
        print("=" * 60)
        print("ğŸ“ ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ìˆ˜ë™ í…ìŠ¤íŠ¸ í•œ ë²ˆ ê²Œì‹œ)")
        print("=" * 60)
        token = get_token()
        user = me(token=token)
        uid = user["id"]
        print("âœ… me:", user)

        result = _post_text_to_threads(uid, "Hello from API âœ¨", token)
        print("ğŸš€ published media_id:", result["media_id"])
        print("ğŸ”— permalink:", result["permalink"])

        print("\nğŸ’¡ AIë¡œ ìƒì„±í•˜ë ¤ë©´: python post_to_threads.py \"ì£¼ì œ\" [--model gpt-4o|gemini-2.5-flash]")
