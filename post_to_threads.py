"""
âœ¨ í”¼ë”ë¦° (PYDERIN): í”¼ë¶€ ê³¼í•™ ê¸°ë°˜ PDRN ì „ë¬¸ ë”ë§ˆ ì†”ë£¨ì…˜

1. ë¸Œëœë“œ ê°œìš” ë° ë¯¸ì…˜

í•­ëª©ë‚´ìš©ë¸Œëœë“œëª…PYDERIN (í”¼ë”ë¦°)ë¸Œëœë“œ ì •ì²´ì„±í”¼ë¶€ ë³¸ì—°ì˜ ê±´ê°•í•œ íšŒë³µì— ì§‘ì¤‘í•˜ëŠ” PDRN ì „ë¬¸ ë”ë§ˆ ì½”ìŠ¤ë©”í‹± ë¸Œëœë“œ (Neo-Regeneration)í•µì‹¬ ë¯¸ì…˜ì˜ë£Œ ì „ë¬¸ì„±ê³¼ ì„±ë¶„ íˆ¬ëª…ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ, ì¶•ì ëœ í”¼ë¶€ ê³¼í•™ ê¸°ìˆ ë ¥ì„ ì¼ìƒ ì† í”¼ë¶€ ì¼€ì–´ë¡œ ì—°ê²°í•˜ì—¬ ê±´ê°•í•œ ì•„ë¦„ë‹¤ì›€ì„ ì™„ì„±í•©ë‹ˆë‹¤.2. í”¼ë”ë¦°ì˜ ì°¨ë³„í™”ëœ í•µì‹¬ ê°€ì¹˜ (The Core Value)

í”¼ë”ë¦°ì€ ë‹¨ìˆœí•œ í™”ì¥í’ˆì„ ë„˜ì–´, **'ê³¼í•™ì ìœ¼ë¡œ ê²€ì¦ëœ í”¼ë¶€ íšŒë³µ ì†”ë£¨ì…˜'**ì„ ì œê³µí•©ë‹ˆë‹¤.

ğŸ¥‡ ê³ ìˆœë„ PDRN (í•µì‹¬ ì„±ë¶„)

ì„±ë¶„ íˆ¬ëª…ì„±: ëŒ€í•œë¯¼êµ­ ì²œì—°ì–´ì—ì„œ ì—„ì„ í•˜ì—¬ ì¶”ì¶œí•œ 99% ê³ ìˆœë„ PDRNë§Œì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

PDRNì˜ ì—­í• : í”¼ë¶€ì— ìƒˆë¡œìš´ ìƒëª… ì—ë„ˆì§€ë¥¼ ë¶ˆì–´ë„£ì–´ í”¼ë¶€ ì¬ìƒì˜ ë³¸ì§ˆì— ì§‘ì¤‘í•˜ê³ , ì†ìƒëœ í”¼ë¶€ì˜ ê±´ê°•í•œ íšŒë³µì„ ë•ìŠµë‹ˆë‹¤.

ğŸ”¬ ì´ˆì €ë¶„ì ê³¼í•™ ê¸°ìˆ  (ì „ë‹¬ë ¥)

97kDa ì´ˆì €ë¶„ì ê¸°ìˆ : ìœ íš¨ ì„±ë¶„ì¸ PDRNì„ í”¼ë¶€ ê¹Šì€ ê³³ê¹Œì§€ íš¨ìœ¨ì ìœ¼ë¡œ ì „ë‹¬í•˜ê¸° ìœ„í•´ ì´ˆì €ë¶„ìí™” ê¸°ìˆ ì„ ì ìš©í–ˆìŠµë‹ˆë‹¤.

ìµœì ì˜ í¡ìˆ˜: í”¼ë¶€ ì»¨ë””ì…˜ì„ ê· í˜• ìˆê²Œ ê´€ë¦¬í•˜ê³ , ì„±ë¶„ì˜ íš¨ëŠ¥ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.

ğŸ§‘â€âš•ï¸ ì˜ë£Œ ê¸°ë°˜ ì†”ë£¨ì…˜ (ì‹ ë¢°ì„±)

ê²€ì¦ëœ ë°ì´í„°: êµ­ë‚´ì™¸ ìš°ìˆ˜ ì—°êµ¬ê¸°ê´€ê³¼ í˜‘ë ¥í•˜ì—¬ ê²€ì¦ëœ ì„ìƒ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì œí’ˆì„ ê°œë°œí•©ë‹ˆë‹¤.

ì „ë¬¸ì„±: ì˜ë£Œ í˜„ì¥ì—ì„œ ì•ˆì •ì„±ê³¼ íšŒë³µë ¥ì´ ê²€ì¦ëœ ê¸°ìˆ ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ, ì¼ìƒì—ì„œë„ ì „ë¬¸ì ì¸ ë”ë§ˆ ì†”ë£¨ì…˜ì„ ê²½í—˜í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.

3. ì£¼ìš” ì œí’ˆ ë¼ì¸ì—… (ì˜ˆì‹œ)

í”¼ë”ë¦°ì€ ì „ë¬¸ì ì¸ ê¸°ìˆ ë ¥ì„ ë‹´ì•„ í”¼ë¶€ ê³ ë¯¼ë³„ ìµœì í™”ëœ í† íƒˆ ìŠ¤í‚¨ì¼€ì–´ ë¼ì¸ì„ ì œê³µí•©ë‹ˆë‹¤.

Hospital Line: ì˜ë£Œ í˜„ì¥ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì „ë¬¸ ì œí’ˆêµ° (ë³‘ì› ì „ìš©)

Daily Care Line: ì¼ìƒ ì† í”¼ë¶€ ì»¨ë””ì…˜ì„ ê´€ë¦¬í•˜ëŠ” ë°ì¼ë¦¬ ì†”ë£¨ì…˜

PDRN íƒ„ë ¥ í• ì•°í”Œ: ê³ ìˆœë„ PDRNì„ ë‹´ì•„ ì§‘ì¤‘ì ì¸ íƒ„ë ¥ ë° ì¬ìƒ ê´€ë¦¬ë¥¼ ë•ëŠ” í•µì‹¬ ì œí’ˆ.

PDRN íƒ„ë ¥ í¬ë¦¼: í”¼ë¶€ ì¥ë²½ ê°•í™” ë° íƒ„ë ¥ ìœ ì§€ì— ë„ì›€ì„ ì£¼ëŠ” ê³ ë³´ìŠµ í¬ë¦¼.

PDRN ë§ˆìŠ¤í¬íŒ©: ì§‘ì¤‘ì ì¸ ì˜ì–‘ ê³µê¸‰ ë° í”¼ë¶€ íšŒë³µì„ ìœ„í•œ ë§ˆìŠ¤í¬íŒ© (ì†/ë°œ ë§ˆìŠ¤í¬íŒ© í¬í•¨).

PDRN í´ë Œì§•: ìˆœí•˜ë©´ì„œë„ íš¨ê³¼ì ì¸ ì„¸ì •ì„ ë•ëŠ” ê±°í’ˆ í´ë Œì €.



ì´ ë¶€ë¶„ì„ AI AX ì „ë¬¸ê°€ë¡œ ë°”ê¿€ê±´ë° ë‹¤ì‹œ ì˜ í”„ë¡¬í”„íŠ¸ë¡œ ì“¸ ìˆ˜ ìˆê²Œ ì •ë¦¬í•´ì„œ ë‚˜í•œí…Œ ì•Œë ¤ì¤˜



ì£¼ì œ => AI AX ì „ë¬¸ê°€ ê¸°ì¤€ì—ì„œì˜ ê´€ì :

ë°°ê²½ : K-ë·°í‹° ë¸Œëœë“œì—ì„œ íšŒì‚¬ ë‚´ë¶€ ì—…ë¬´ë¥¼ ê°•ë ¥í•˜ê²Œ AX ì¶”ì§„ ì¤‘ì¸ ìƒí™©

1) ì§€ê¸ˆê¹Œì§€ ì§„í–‰ ì™„ë£Œ
 - 


"""


import json
import os
import time
import sys
from typing import Callable, List, Optional

import requests
from dotenv import load_dotenv
from openai import OpenAI

try:
    from google import genai as google_genai
    GOOGLE_GENAI_AVAILABLE = True
except ImportError:
    GOOGLE_GENAI_AVAILABLE = False
    google_genai = None

# Windows ì½˜ì†” UTF-8 ì¸ì½”ë”© ì„¤ì •
if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except AttributeError:
        # Python 3.6 ì´í•˜ ë²„ì „ì„ ìœ„í•œ ëŒ€ì²´ ë°©ë²•
        import codecs
        sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
        sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

BASE = "https://graph.threads.net/v1.0"

Logger = Optional[Callable[[str], None]]


def _emit(message: str, logger: Logger = None) -> None:
    """Helper to send messages to either stdout or a provided logger."""
    if logger:
        logger(message)
    else:
        print(message)

def get_token():
    """í™˜ê²½ ë³€ìˆ˜ì—ì„œ í† í°ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    token = os.getenv('LONG_LIVED_ACCESS_TOKEN')
    if not token:
        raise ValueError("LONG_LIVED_ACCESS_TOKENì´ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    return token.strip().strip('"').strip("'")  # ë”°ì˜´í‘œ ì œê±°

def get_google_api_key():
    """í™˜ê²½ ë³€ìˆ˜ì—ì„œ Google API í‚¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    api_key = os.getenv('GOOGLE_API_KEY')
    if not api_key:
        raise ValueError("GOOGLE_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    return api_key.strip().strip('"').strip("'")

class ContentGenerator:
    def __init__(self, model="gemini-2.5-flash", logger: Logger = None):
        self.model = model
        self.logger = logger
        self.history = [] # For GPT
        self.gemini_chat = None # For Gemini
        
        if model.startswith("gemini"):
            if not GOOGLE_GENAI_AVAILABLE:
                raise ImportError("google-genai ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            api_key = get_google_api_key()
            client = google_genai.Client(api_key=api_key)
            # Gemini chat session initialization
            self.gemini_chat = client.chats.create(model="gemini-2.5-flash")
            
        elif model.startswith("gpt"):
            self.api_key = os.getenv('OPENAI_API_KEY')
            if not self.api_key:
                raise ValueError("OPENAI_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            self.client = OpenAI(api_key=self.api_key.strip().strip('"').strip("'"))
            # System prompt can be set initially if needed, but we'll just start empty
            self.history = [
                {"role": "system", "content": "ë‹¹ì‹ ì€ SNS ì¹´í”¼ë¼ì´íŒ… ì „ë¬¸ê°€ì…ë‹ˆë‹¤. Meta Threadsì— ìµœì í™”ëœ ë°˜ë§/êµ¬ì–´ì²´ ê¸€ì„ ì‘ì„±í•©ë‹ˆë‹¤."}
            ]

    def generate(self, prompt: str) -> str:
        if self.model.startswith("gemini"):
            return self._generate_gemini(prompt)
        else:
            return self._generate_gpt(prompt)

    def _generate_gemini(self, prompt: str) -> str:
        try:
            response = self.gemini_chat.send_message(prompt)
            content = response.text.strip()
            content = self._clean_content(content)
            _emit(f"âœ… Gemini ìƒì„± ì™„ë£Œ ({len(content)}ì)", self.logger)
            return content
        except Exception as e:
            _emit(f"âŒ Gemini ì˜¤ë¥˜: {e}", self.logger)
            raise

    def _generate_gpt(self, prompt: str) -> str:
        try:
            self.history.append({"role": "user", "content": prompt})
            
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=self.history,
                temperature=0.7,
                max_tokens=500
            )
            
            content = response.choices[0].message.content.strip()
            content = self._clean_content(content)
            
            # Add assistant response to history
            self.history.append({"role": "assistant", "content": content})
            
            _emit(f"âœ… GPT ìƒì„± ì™„ë£Œ ({len(content)}ì)", self.logger)
            return content
        except Exception as e:
            _emit(f"âŒ GPT ì˜¤ë¥˜: {e}", self.logger)
            raise

    def _clean_content(self, content: str) -> str:
        if content.startswith('"') and content.endswith('"'):
            return content[1:-1]
        elif content.startswith("'") and content.endswith("'"):
            return content[1:-1]
        return content

# Legacy wrapper for backward compatibility if needed, or just remove
def generate_text_with_ai(model="gpt-4o", prompt=None, logger: Logger = None):
    generator = ContentGenerator(model=model, logger=logger)
    return generator.generate(prompt)

def me(token=None):
    if token is None:
        token = get_token()
    r = requests.get(f"{BASE}/me", params={"fields":"id,username","access_token":token}, timeout=20)
    r.raise_for_status()
    return r.json()

def create_text_container(threads_user_id, text, token=None):
    if token is None:
        token = get_token()
    payload = {"media_type":"TEXT", "text": text, "access_token": token}
    r = requests.post(f"{BASE}/{threads_user_id}/threads",
                      headers={"Content-Type":"application/json"},
                      data=json.dumps(payload), timeout=30)
    r.raise_for_status()
    return r.json()["id"]  # creation_id

def publish_container(threads_user_id, creation_id, token=None):
    if token is None:
        token = get_token()
    r = requests.post(f"{BASE}/{threads_user_id}/threads_publish",
                      data={"creation_id": creation_id, "access_token": token}, timeout=20)
    r.raise_for_status()
    return r.json()["id"]  # ìµœì¢… media id

def get_permalink(media_id, token=None):
    if token is None:
        token = get_token()
    r = requests.get(f"{BASE}/{media_id}", params={"fields":"permalink","access_token":token}, timeout=20)
    r.raise_for_status()
    return r.json()["permalink"]

def _post_text_to_threads(threads_user_id: str, text: str, token: str, logger: Logger = None):
    """Create, publish, and return metadata for a single Threads post."""
    _emit("ğŸ“¦ ì»¨í…Œì´ë„ˆ ìƒì„± ì¤‘...", logger)
    creation_id = create_text_container(threads_user_id, text, token=token)

    _emit("ğŸš€ Threadsì— ê²Œì‹œ ì¤‘...", logger)
    media_id = publish_container(threads_user_id, creation_id, token=token)

    _emit("ğŸ”— Permalink ê°€ì ¸ì˜¤ëŠ” ì¤‘...", logger)
    permalink = get_permalink(media_id, token=token)

    return {
        "media_id": media_id,
        "creation_id": creation_id,
        "permalink": permalink,
        "text": text,
        "user_id": threads_user_id,
    }


def post_gpt_generated_text(
    topic=None,
    model="gpt-4o",
    token=None,
    logger: Logger = None
):
    """
    AI ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê³  Threadsì— ê²Œì‹œí•˜ëŠ” ì „ì²´ í”Œë¡œìš°ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    
    Args:
        topic (str, optional): AIê°€ ìƒì„±í•  ì½˜í…ì¸ ì˜ ì£¼ì œ
        model (str): ì‚¬ìš©í•  AI ëª¨ë¸ ("gpt-4o" ë˜ëŠ” "gemini-2.5-flash")
        token (str, optional): Threads ì•¡ì„¸ìŠ¤ í† í° (Noneì´ë©´ .envì—ì„œ ì½ìŒ)
        logger (Logger, optional): ë¡œê·¸ í•¨ìˆ˜
    
    Returns:
        dict: ê²Œì‹œ ê²°ê³¼ (media_id, permalink ë“± í¬í•¨)
    """
    # 1ë‹¨ê³„: AIë¡œ í…ìŠ¤íŠ¸ ìƒì„±
    model_name = "GPT" if model.startswith("gpt") else "Gemini"
    _emit(f"ğŸ¤– {model_name}ë¡œ í…ìŠ¤íŠ¸ ìƒì„± ì¤‘...", logger)
    text = generate_text_with_ai(model=model, topic=topic, logger=logger)
    _emit(f"ìƒì„±ëœ í…ìŠ¤íŠ¸: {text[:100]}...", logger)

    # 2ë‹¨ê³„: Threads ì‚¬ìš©ì ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    if token is None:
        token = get_token()
    _emit("ğŸ“‹ Threads ì‚¬ìš©ì ì •ë³´ í™•ì¸ ì¤‘...", logger)
    user_info = me(token=token)
    threads_user_id = user_info["id"]
    username = user_info.get('username', 'N/A')
    _emit(f"ì‚¬ìš©ì ID: {threads_user_id} (@{username})", logger)

    # 3~5ë‹¨ê³„: ê²Œì‹œ ë° ë§í¬ ë°˜í™˜
    result = _post_text_to_threads(threads_user_id, text, token, logger=logger)

    _emit("\nâœ… ê²Œì‹œ ì™„ë£Œ!", logger)
    _emit(f"ğŸ“ Media ID: {result['media_id']}", logger)
    _emit(f"ğŸ”— Permalink: {result['permalink']}", logger)

    return result


def post_multiple_gpt_texts(
    topic=None,
    count=5,
    interval_seconds=60,
    model="gpt-4o",
    token=None,
    logger: Logger = None,
) -> List[dict]:
    """
    ì§€ì •ëœ íšŸìˆ˜ë§Œí¼ Threadsì— AI ìƒì„± ê²Œì‹œë¬¼ì„ ìˆœì°¨ì ìœ¼ë¡œ ì—…ë¡œë“œí•©ë‹ˆë‹¤.

    Args:
        topic (str, optional): ê° ê²Œì‹œë¬¼ì— ëŒ€í•œ í”„ë¡¬í”„íŠ¸ ì£¼ì œ.
        count (int): ê²Œì‹œí•  ê²Œì‹œë¬¼ ìˆ˜.
        interval_seconds (int): ê²Œì‹œ ì‚¬ì´ ì§€ì—°(ì´ˆ).
        model (str): ì‚¬ìš©í•  AI ëª¨ë¸ ("gpt-4o" ë˜ëŠ” "gemini-2.5-flash").
        token (str, optional): Threads ì•¡ì„¸ìŠ¤ í† í°.
        logger (callable, optional): ë¡œê·¸ ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•  ì½œë°±.

    Returns:
        List[dict]: ê° ê²Œì‹œë¬¼ì˜ ê²°ê³¼ ì •ë³´ ëª©ë¡.
    """
    if count < 1:
        raise ValueError("countëŠ” 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")

    if interval_seconds < 0:
        raise ValueError("interval_secondsëŠ” 0 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")

    if token is None:
        token = get_token()

    _emit("ğŸ“‹ Threads ì‚¬ìš©ì ì •ë³´ í™•ì¸ ì¤‘...", logger)
    user_info = me(token=token)
    threads_user_id = user_info["id"]
    username = user_info.get('username', 'N/A')
    _emit(f"ì‚¬ìš©ì ID: {threads_user_id} (@{username})", logger)

    model_name = "GPT" if model.startswith("gpt") else "Gemini"
    results = []

    for idx in range(count):
        _emit(f"\n===== ê²Œì‹œ {idx + 1}/{count} ì‹œì‘ =====", logger)
        _emit(f"ğŸ¤– {model_name}ë¡œ í…ìŠ¤íŠ¸ ìƒì„± ì¤‘...", logger)
        text = generate_text_with_ai(model=model, topic=topic, logger=logger)
        _emit(f"ìƒì„±ëœ í…ìŠ¤íŠ¸: {text[:100]}...", logger)

        result = _post_text_to_threads(threads_user_id, text, token, logger=logger)
        result["sequence"] = idx + 1
        result["username"] = username
        results.append(result)

        _emit(f"âœ… ê²Œì‹œ {idx + 1}/{count} ì™„ë£Œ! Permalink: {result['permalink']}", logger)

        if idx < count - 1 and interval_seconds > 0:
            _emit(f"â³ ë‹¤ìŒ ê²Œì‹œê¹Œì§€ {interval_seconds}ì´ˆ ëŒ€ê¸°í•©ë‹ˆë‹¤...", logger)
            time.sleep(interval_seconds)

    _emit("\nğŸ‰ ëª¨ë“  ê²Œì‹œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!", logger)
    return results

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Threadsì— AI ìƒì„± ì½˜í…ì¸ ë¥¼ ìë™ ê²Œì‹œí•©ë‹ˆë‹¤.")
    parser.add_argument("topic", nargs="?", help="AIê°€ ìƒì„±í•  ì½˜í…ì¸  ì£¼ì œ (ë¯¸ì…ë ¥ ì‹œ ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì‹¤í–‰)")
    parser.add_argument("--count", type=int, default=5, help="ê²Œì‹œí•  ê²Œì‹œë¬¼ ìˆ˜ (ê¸°ë³¸ê°’: 5)")
    parser.add_argument("--interval", dest="interval_seconds", type=int, default=60, help="ê²Œì‹œ ê°„ê²©(ì´ˆ) (ê¸°ë³¸ê°’: 60)")
    parser.add_argument("--model", default="gpt-4o", choices=["gpt-4o", "gemini-2.5-flash"], help="ì‚¬ìš©í•  AI ëª¨ë¸ (ê¸°ë³¸ê°’: gpt-4o)")
    args = parser.parse_args()

    if args.topic:
        print(f"ğŸ¯ ì£¼ì œ: {args.topic}")
        print(f"ğŸ¤– ëª¨ë¸: {args.model}")
        print("=" * 60)
        post_multiple_gpt_texts(
            topic=args.topic,
            count=args.count,
            interval_seconds=args.interval_seconds,
            model=args.model,
        )
    else:
        print("=" * 60)
        print("ğŸ“ ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ìˆ˜ë™ í…ìŠ¤íŠ¸ í•œ ë²ˆ ê²Œì‹œ)")
        print("=" * 60)
        token = get_token()
        user = me(token=token)
        uid = user["id"]
        print("âœ… me:", user)

        result = _post_text_to_threads(uid, "Hello from API âœ¨", token)
        print("ğŸš€ published media_id:", result["media_id"])
        print("ğŸ”— permalink:", result["permalink"])

        print("\nğŸ’¡ AIë¡œ ìƒì„±í•˜ë ¤ë©´: python post_to_threads.py \"ì£¼ì œ\" [--model gpt-4o|gemini-2.5-flash]")
